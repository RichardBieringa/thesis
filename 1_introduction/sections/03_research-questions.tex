\section{Main Research Questions}
\label{sec:introduction:research-questions}

We decompose the challenges as described in the problem statement \cref{sec:problem-statement} in the following research questions.


% Research Questions
% Map the research questions to the chapters/contributions
% 1 RQ per chapter
% - Systems Survey 
% - Design/implementation of a benchmarking system
% - Real World Experiments

\todo{Research Questions: Formulation}

\begin{enumerate}[label=\textbf{RQ\arabic*}, leftmargin=3\parindent]
    \item \textbf{How to compare, and evaluate \gls{sm} systems?}
    \label{rq-1}
    
    Many \gls{sm} implementations have emerged from within the rapidly changing landscape of container and resource management technologies, before the adaptation of any standard to guide their development. It is necessary to evaluate these implementations in a structured systems survey, where we identify and analyse the characteristics of the current iterations of \gls{sm} systems. This allows us to create an overview of the state-of-the-art within the \gls{sm} landscape.
    
    \item \textbf{How to design and implement a benchmark that evaluates the performance of \gls{sm} systems?}
    \label{rq-2}
    
    Based on the results of the system survey in \ref{rq-1}, we will identify the requirements for a system which can evaluate a service mesh. Based on these identified requirements, we should evaluate existing instruments to see if any of these satisfy. We then have to design or extend an instrument based on best practices \cite{folkerts2012benchmarking} which can support multiple experiment configuration, workloads and can produce reproducible real-world experiments. The goal of this instrument is to capture the relevant system metrics, performance metrics and application domain specific \glspl{nfr} which results in a generic and objective comparison between said systems.

    \item \textbf{What are the differences between the different \gls{sm} systems in terms of overhead, throughput and latency?}
    \label{rq-3}
    
    Based on the benchmark as designed in \ref{rq-2}, we have to conduct real-world experiments on different \gls{sm} systems. The benchmark should compare popular \gls{sm} implementations and implementations that vary based on their architectural design. The benchmark should compare different application workloads, highlighting various application-specific aspects of the system where different implementations can vary in performance or capabilities. Finally, the benchmark should be as systematic as possible, meaning that anyone can verify the results by reproducing the experiments bearing in mind the inherent nature of variance within distributed systems.

\end{enumerate}

% Old ones
% \subsection{Research Questions}
% \begin{enumerate}
%   \item What does the term 'service mesh' mean in the context of distributed systems?
%   \item What are the defining properties and characteristics that define the service mesh?
%   \item How can we model the performance and reliability characteristics of a service mesh?
%   \item How can we conduct real world experiments on different service mesh implementations to objectively compare them?
% \end{enumerate}

